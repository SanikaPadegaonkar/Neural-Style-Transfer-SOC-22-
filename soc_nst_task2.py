# -*- coding: utf-8 -*-
"""SOC_NST_Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1scvzsMoAogmUuxYLLnsv8mJhY7ce6XYP
"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, LeakyReLU, Bidirectional
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
import re
from nltk.corpus import stopwords
from nltk import word_tokenize
import nltk
nltk.download('stopwords')
STOPWORDS = set(stopwords.words('english'))

#uploading the datasets
from google.colab import files
uploaded=files.upload()
df_train=pd.read_csv('train.csv')
df_test=pd.read_csv('test.csv')

print(df_train)
print(df_test)

authors=df_train['author'].unique()
print(authors)

df_train = df_train.reset_index(drop=True)
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = text.replace('x', '')
#    text = re.sub(r'\W+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text
df_train['text'] = df_train['text'].apply(clean_text)

df_train['text'] = df_train['text'].str.replace('\d+', '') #DOUBT:Remove numbers?

# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 250
# This is fixed.
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df_train['text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(df_train['text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

Y = pd.get_dummies(df_train['author']).values
print('Shape of label tensor:', Y.shape)

X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y, test_size = 0.20, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_valid.shape,Y_valid.shape)
#X_train=X
#Y_train=Y

"""#**1 Dense Layer**

##Epochs=5 and Batch Size=64
"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""##Epochs=3 and batch size=64"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 3
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""## Epochs=2 and Batch size=64"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 2
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""#**1 Dense Layer without Softmax Activation function (Only LeakyReLU)**

##Epochs=5
"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""We can observe that values deteriorate after that third epoch so we'll try training for 3 epochs only

##Epochs=3
"""

epochs = 3
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

#df_test['text'] = df_test['text'].apply(clean_text)
#df_test['text'] = df_test['text'].str.replace('\d+', '')
#new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']
X_test = tokenizer.texts_to_sequences(df_test['text'].values)
#seq = tokenizer.texts_to_sequences(new_complaint)
X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
#padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
predictions = model.predict(X_test)
#print(predictions[:5])
#print(np.argmax(predictions[55]))
#labels = authors
df_test_out=df_test
pred_authors=[]
for pred in predictions:
  pred_authors.append(authors[np.argmax(pred)])
df_test_out['Predicted Authors']=pred_authors
#print(pred, labels[np.argmax(pred)])
print(df_test_out)
df_test_out.to_csv('predicted_authors_Task2')

"""# **5 Dense Layers**"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(64, activation='softmax'))
model.add(Dense(32, activation='softmax'))
model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

#df_test['text'] = df_test['text'].apply(clean_text)
#df_test['text'] = df_test['text'].str.replace('\d+', '')
#new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']
X_test = tokenizer.texts_to_sequences(df_test['text'].values)
#seq = tokenizer.texts_to_sequences(new_complaint)
X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
#padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
predictions = model.predict(X_test)
#print(predictions[:5])
#print(np.argmax(predictions[55]))
#labels = authors
df_test_out=df_test
pred_authors=[]
for pred in predictions:
  pred_authors.append(authors[np.argmax(pred)])
df_test_out['Predicted Authors']=pred_authors
#print(pred, labels[np.argmax(pred)])
print(df_test_out)
df_test_out.to_csv('predicted_authors_Task2')

"""**Conclusion:** Loss and accuracy improve but loss is too high and accuracy is too low at the end of the 5th epoch

# **5 Dense Layers without Softmax Activation function (Only LeakyReLU)**
"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(64))
model.add(Dense(32))
model.add(Dense(16))
model.add(Dense(3))
model.add(Dense(3))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

#df_test['text'] = df_test['text'].apply(clean_text)
#df_test['text'] = df_test['text'].str.replace('\d+', '')
#new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']
X_test = tokenizer.texts_to_sequences(df_test['text'].values)
#seq = tokenizer.texts_to_sequences(new_complaint)
X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
#padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
predictions = model.predict(X_test)
#print(predictions[:5])
#print(np.argmax(predictions[55]))
#labels = authors
df_test_out=df_test
pred_authors=[]
for pred in predictions:
  pred_authors.append(authors[np.argmax(pred)])
df_test_out['Predicted Authors']=pred_authors
#print(pred, labels[np.argmax(pred)])
print(df_test_out)
df_test_out.to_csv('predicted_authors_Task2')

"""**Conclusion:** Loss is too high and accuracy too low to begin with and they deteriorate with each epoch; so much that the model stops training due to early stopping because of increasing val_loss.

# **BiLSTMs**

##Epochs=5
"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 5
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

"""##Epochs=3"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))
#model.add(Dense(64, activation='softmax'))
#model.add(Dense(32, activation='softmax'))
#model.add(Dense(16, activation='softmax'))
model.add(Dense(3, activation='softmax'))
model.add(LeakyReLU(alpha=0.05))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

epochs = 3
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

#df_test['text'] = df_test['text'].apply(clean_text)
#df_test['text'] = df_test['text'].str.replace('\d+', '')
#new_complaint = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']
X_test = tokenizer.texts_to_sequences(df_test['text'].values)
#seq = tokenizer.texts_to_sequences(new_complaint)
X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
#padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
predictions = model.predict(X_test)
#print(predictions[:5])
#print(np.argmax(predictions[55]))
#labels = authors
df_test_out=df_test
pred_authors=[]
for pred in predictions:
  pred_authors.append(authors[np.argmax(pred)])
df_test_out['Predicted Authors']=pred_authors
#print(pred, labels[np.argmax(pred)])
print(df_test_out)
df_test_out.to_csv('predicted_authors_Task2')

"""#**Conclusion:** 

1.   LSTM with 1 Dense Layer with Softmax activation for Dense layers and LeakyReLU activation function gives the best validation loss and accuracy in 3 epochs. [val_loss=0.5653, val_accuracy=0.8053]
2.   Second Best: BiLSTM with 1 Dense Layer with Softmax activation for Dense layers and LeakyReLU activation function in 3 epochs [val_loss=0.5592, val_accuracy=0.8031]


"""